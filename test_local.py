#!/usr/bin/env python3
"""
Test script cho VoiceSub-Translator local environment
"""

import sys
import os

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """Test basic imports"""
    print("=== Testing Basic Imports ===")
    
    try:
        import torch
        print(f"‚úÖ PyTorch: {torch.__version__}")
        print(f"‚úÖ CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"‚úÖ GPU device: {torch.cuda.get_device_name()}")
            print(f"‚úÖ GPU count: {torch.cuda.device_count()}")
    except Exception as e:
        print(f"‚ùå PyTorch error: {e}")
    
    try:
        import whisper
        print(f"‚úÖ OpenAI Whisper imported")
    except Exception as e:
        print(f"‚ùå Whisper error: {e}")
    
    try:
        from faster_whisper import WhisperModel
        print(f"‚úÖ Faster Whisper imported")
    except Exception as e:
        print(f"‚ùå Faster Whisper error: {e}")
    
    try:
        import openai
        print(f"‚úÖ OpenAI: {openai.__version__}")
    except Exception as e:
        print(f"‚ùå OpenAI error: {e}")

def test_project_structure():
    """Test project structure"""
    print("\n=== Testing Project Structure ===")
    
    required_dirs = ['src', 'src/gui', 'src/api', 'src/processor', 'src/translator']
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"‚úÖ {dir_path}")
        else:
            print(f"‚ùå {dir_path} - missing")

def test_gpu_functionality():
    """Test GPU functionality with simple tensor operation"""
    print("\n=== Testing GPU Functionality ===")
    
    try:
        import torch
        if torch.cuda.is_available():
            # Create tensors on GPU
            device = torch.device('cuda')
            x = torch.randn(100, 100).to(device)
            y = torch.randn(100, 100).to(device)
            z = torch.matmul(x, y)
            print(f"‚úÖ GPU tensor operations successful")
            print(f"‚úÖ Result tensor shape: {z.shape}")
            print(f"‚úÖ GPU memory used: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
        else:
            print("‚ùå CUDA not available")
    except Exception as e:
        print(f"‚ùå GPU test error: {e}")

def main():
    """Main test function"""
    print("üß™ VoiceSub-Translator Local Environment Test")
    print("=" * 50)
    
    test_imports()
    test_project_structure() 
    test_gpu_functionality()
    
    print("\n" + "=" * 50)
    print("‚ú® Test completed!")

if __name__ == "__main__":
    main()
